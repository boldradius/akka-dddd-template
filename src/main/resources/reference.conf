akka {
  loggers = [akka.event.slf4j.Slf4jLogger]
  loglevel = INFO

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
  }

  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "127.0.0.1"
      port = 0
    }
  }

  remote.watch-failure-detector.threshold = 20

  cluster {
    seed-nodes = [
      "akka.tcp://ClusterSystem@127.0.0.1:2551",
      "akka.tcp://ClusterSystem@127.0.0.1:2552"]

    auto-down-unreachable-after = off
  }

  persistence {


    journal {
      max-message-batch-size = 200
      max-confirmation-batch-size = 10000
      max-deletion-batch-size = 10000
      plugin = "cassandra-journal"
    }
    snapshot-store {
      plugin = "cassandra-snapshot-store"
    }


    #journal.plugin = "akka.persistence.journal.leveldb-shared"
    #journal.leveldb-shared.store {
    # DO NOT USE 'native = off' IN PRODUCTION !!!
    #native = off
    #dir = "target/shared-journal"
    #}
    #snapshot-store.local.dir = "target/snapshots"
    #view.auto-update-interval = 2s
  }

  contrib.cluster.sharding {
    # The extension creates a top level actor with this name in top level user scope,
    # e.g. '/user/sharding'
    guardian-name = sharding
    # If the coordinator can't store state changes it will be stopped
    # and started again after this duration.
    coordinator-failure-backoff = 1 s
    # Start the coordinator singleton manager on members tagged with this role.
    # All members are used if undefined or empty.
    # ShardRegion actor is started in proxy only mode on nodes that are not tagged
    # with this role.
    role = ""
    # The ShardRegion retries registration and shard location requests to the
    # ShardCoordinator with this interval if it does not reply.
    retry-interval = 1 s
    # Maximum number of messages that are buffered by a ShardRegion actor.
    buffer-size = 100000
    # Timeout of the shard rebalancing process.
    handoff-timeout = 60 s
    # Time given to a region to acknowdge it's hosting a shard.
    shard-start-timeout = 10 s
    # If the shard can't store state changes it will retry the action
    # again after this duration. Any messages sent to an affected entry
    # will be buffered until the state change is processed
    shard-failure-backoff = 10 s
    # If the shard is remembering entries and an entry stops itself without
    # using passivate. The entry will be restarted after this duration or when
    # the next message for it is received, which ever occurs first.
    entry-restart-backoff = 10 s
    # Rebalance check is performed periodically with this interval.
    rebalance-interval = 10 s
    # How often the coordinator saves persistent snapshots, which are
    # used to reduce recovery times
    snapshot-interval = 3600 s
    # Setting for the default shard allocation strategy
    least-shard-allocation-strategy {
      # Threshold of how large the difference between most and least number of
      # allocated shards must be to begin the rebalancing.
      rebalance-threshold = 10
      # The number of ongoing rebalancing processes is limited to this number.
      max-simultaneous-rebalance = 3
    }
  }


}


cassandra-journal {
  # FQCN of the cassandra journal plugin
  class = "akka.persistence.cassandra.journal.CassandraJournal"

  # Comma-separated list of contact points in the cluster
  contact-points = ["127.0.0.1"]

  # Port of contact points in the cluster
  port = 9042

  # Name of the keyspace to be created/used by the journal
  keyspace = "akka_dddd_template_journal"

  # Name of the table to be created/used by the journal
  table = "akka_dddd_template_journal"

  # Replication factor to use when creating a keyspace
  replication-factor = 1

  # Write consistency level
  write-consistency = "QUORUM"

  # Read consistency level
  read-consistency = "QUORUM"

  # Maximum number of entries per partition (= columns per row).
  # Must not be changed after table creation (currently not checked).
  max-partition-size = 5000000

  # Maximum size of result set
  max-result-size = 50001

  # Dispatcher for the plugin actor.
  plugin-dispatcher = "akka.actor.default-dispatcher"

  # Dispatcher for fetching and replaying messages
  replay-dispatcher = "akka.persistence.dispatchers.default-replay-dispatcher"
}

cassandra-snapshot-store {

  # FQCN of the cassandra snapshot store plugin
  class = "akka.persistence.cassandra.snapshot.CassandraSnapshotStore"

  # Comma-separated list of contact points in the cluster
  contact-points = ["127.0.0.1"]

  # Port of contact points in the cluster
  port = 9042

  # Name of the keyspace to be created/used by the snapshot store
  keyspace = "akka_dddd_template_snapshot"

  # Name of the table to be created/used by the snapshot store
  table = "akka_dddd_template_snapshot"

  # Replication factor to use when creating a keyspace
  replication-factor = 1

  # Write consistency level
  write-consistency = "ONE"

  # Read consistency level
  read-consistency = "ONE"

  # Maximum number of snapshot metadata to load per recursion (when trying to
  # find a snapshot that matches specified selection criteria). Only increase
  # this value when selection criteria frequently select snapshots that are
  # much older than the most recent snapshot i.e. if there are much more than
  # 10 snapshots between the most recent one and selected one. This setting is
  # only for increasing load efficiency of snapshots.
  max-metadata-result-size = 10

  # Dispatcher for the plugin actor.
  plugin-dispatcher = "cassandra-snapshot-store.default-dispatcher"

  # Default dispatcher for plugin actor.
  default-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-max = 8
    }
  }
}

cassandra-query-journal {
  # Implementation class of the Cassandra ReadJournalProvider
  class = "akka.persistence.cassandra.query.CassandraReadJournalProvider"

  # Absolute path to the write journal plugin configuration section
  write-plugin = "cassandra-journal"

  # New events are retrieved (polled) with this interval.
  refresh-interval = 3s

  # How many events to fetch in one query (replay) and keep buffered until they
  # are delivered downstreams.
  max-buffer-size = 500

  # The fetch size of the Cassandra select statement
  # Value less or equal to 0 means max-result-size will be used
  # http://docs.datastax.com/en/drivers/java/3.0/com/datastax/driver/core/Statement.html
  max-result-size-query = 250

  # Read consistency level
  read-consistency = "QUORUM"

  # Configure this to the day (yyyyMMdd) when the system was first started.
  # When offset 0L is used it will look for events from this day and forward.
  first-time-bucket = "20151120"

  # The returned event stream is ordered by the offset (timestamp), which corresponds
  # to the same order as the write journal stored the events, with inaccuracy due to clock skew
  # between different nodes. The same stream elements (in same order) are returned for multiple
  # executions of the query on a best effort basis. The query is using a Cassandra Materialized
  # View for the query and that is eventually consistent, so different queries may see different
  # events for the latest events, but eventually the result will be ordered by timestamp
  # (Cassandra timeuuid column). To compensate for the the eventual consistency the query is
  # delayed to not read the latest events, the duration of this delay is defined by this
  # configuration property.
  # However, this is only best effort and in case of network partitions
  # or other things that may delay the updates of the Materialized View the events may be
  # delivered in different order (not strictly by their timestamp).
  eventual-consistency-delay = 10s

  # If you use the same tag for all events for a `persistenceId` it is possible to get
  # a more strict delivery order than otherwise. This can be useful when all events of
  # a PersistentActor class (all events of all instances of that PersistentActor class)
  # are tagged with the same tag. Then the events for each `persistenceId` can be delivered
  # strictly by sequence number. If a sequence number is missing the query is delayed up
  # to the configured `delayed-event-timeout` and if the expected event is still not
  # found the stream is completed with failure. This means that there must not be any
  # holes in the sequence numbers for a given tag, i.e. all events must be tagged
  # with the same tag. Set this property to for example 30s to enable this feature.
  # It is disabled by default.
  delayed-event-timeout = 0s

  # Dispatcher for the plugin actors.
  plugin-dispatcher = "cassandra-plugin-default-dispatcher"

}


# Default dispatcher for plugin actor and tasks.
cassandra-plugin-default-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 8
    parallelism-factor = 1.0
    parallelism-max = 16
  }
}
